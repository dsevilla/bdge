{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NoSQL (Hive & Pig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta hoja es una introducción al uso de Hive y Pig.\n",
    "\n",
    "Utilizaremos la imagen Quickstart de Cloudera.\n",
    "\n",
    "Usaremos la librería `happybase` para python. La cargamos a continuación y hacemos la conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install happybase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import happybase\n",
    "\n",
    "host = 'quickstart.cloudera'\n",
    "connection = happybase.Connection(host)\n",
    "connection.tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la carga inicial, vamos a crear todas las tablas con una única familia de columnas, `rawdata`, donde meteremos toda la información _raw_ comprimida. Después podremos hacer reorganizaciones de los datos para hacer el acceso más eficiente. Es una de las muchas ventajas de no tener un esquema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=../Posts.csv\n",
    "test -e $file || wget http://neuromancer.inf.um.es:8080/es.stackoverflow/`basename ${file}`.gz -O - 2>/dev/null | gunzip > $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=../Users.csv\n",
    "test -e $file || wget http://neuromancer.inf.um.es:8080/es.stackoverflow/`basename ${file}`.gz -O - 2>/dev/null | gunzip > $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=../Tags.csv\n",
    "test -e $file || wget http://neuromancer.inf.um.es:8080/es.stackoverflow/`basename ${file}`.gz -O - 2>/dev/null | gunzip > $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=../Comments.csv\n",
    "test -e $file || wget http://neuromancer.inf.um.es:8080/es.stackoverflow/`basename ${file}`.gz -O - 2>/dev/null | gunzip > $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "file=../Votes.csv\n",
    "test -e $file || wget http://neuromancer.inf.um.es:8080/es.stackoverflow/`basename ${file}`.gz -O - 2>/dev/null | gunzip > $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "tables = ['posts', 'votes', 'users', 'tags', 'comments']\n",
    "for t in tables:\n",
    "    try:\n",
    "        connection.create_table(\n",
    "            t,\n",
    "            {\n",
    "                'rawdata': dict(max_versions=1,compression='GZ')\n",
    "            })\n",
    "    except:\n",
    "        print(\"Database already exists: {0}.\".format(t))\n",
    "        pass\n",
    "connection.tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de importación es siempre el mismo, ya que se coge la primera fila del CSV que contiene el nombre de las columnas y se utiliza para generar nombres de columnas dentro de la familia de columnas dada como parámetro. La función `csv_to_hbase()` acepta un fichero CSV a abrir, un nombre de tabla y una familia de columnas donde agregar las columnas del fichero CSV. En nuestro caso siempre va a ser `rawdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def csv_to_hbase(file, tablename, cf):\n",
    "    table = connection.table(tablename)\n",
    "    \n",
    "    with open(file) as f:\n",
    "        # La llamada csv.reader() crea un iterador sobre un fichero CSV\n",
    "        reader = csv.reader(f, dialect='excel')\n",
    "        \n",
    "        # Se leen las columnas. Sus nombres se usarán para crear las diferentes columnas en la familia\n",
    "        columns = next(reader)\n",
    "        columns = [cf + ':' + c for c in columns]\n",
    "        \n",
    "        with table.batch(batch_size=500) as b:\n",
    "            for row in reader:\n",
    "                # La primera columna se usará como Row Key\n",
    "                b.put(row[0], dict(zip(columns[1:], row[1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tables:\n",
    "    print(\"Importando tabla {0}...\".format(t))\n",
    "    %time csv_to_hbase('../'+t.capitalize() + '.csv', t, 'rawdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = connection.table('posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener el Post con `Id` 5. La orden más sencilla e inmediata de HBase es obtener una fila, opcionalmente limitando las columnas a mostrar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.row(b'5',columns=[b'rawdata:Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código permite mostrar de forma amigable las tablas extraídas de la base de datos en forma de diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/a/30525061/62365\n",
    "class DictTable(dict):\n",
    "    # Overridden dict class which takes a dict in the form {'a': 2, 'b': 3},\n",
    "    # and renders an HTML Table in IPython Notebook.\n",
    "    def _repr_html_(self):\n",
    "        htmltext = [\"<table width=100%>\"]\n",
    "        for key, value in self.items():\n",
    "            htmltext.append(\"<tr>\")\n",
    "            htmltext.append(\"<td>{0}</td>\".format(key.decode('utf-8')))\n",
    "            htmltext.append(\"<td>{0}</td>\".format(value.decode('utf-8')))\n",
    "            htmltext.append(\"</tr>\")\n",
    "        htmltext.append(\"</table>\")\n",
    "        return ''.join(htmltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Muestra cómo queda la fila del Id del Post 9997\n",
    "DictTable(posts.row(b'5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En otra terminal podemos ejecutar, para arrancar un _shell_ dentro del contenedor:\n",
    "\n",
    "```\n",
    "docker exec --user cloudera -ti pighive_quickstart.cloudera_1 bash\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente script carga todos los Posts directamente del fichero `Posts.csv`. Habrá que añadirlo primero desde la interfaz en la pestaña de gestión de ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register '/usr/lib/pig/piggybank.jar';\n",
    "\n",
    "define CSVLoader org.apache.pig.piggybank.storage.CSVLoader();\n",
    "A = LOAD '/user/cloudera/Posts.csv' using CSVLoader \n",
    "   AS (Id:chararray,AcceptedAnswerId:chararray,AnswerCount:chararray,Body:chararray,\n",
    "       ClosedDate:chararray,CommentCount:chararray,CommunityOwnedDate:chararray,\n",
    "       CreationDate:chararray,FavoriteCount:chararray,LastActivityDate:chararray,\n",
    "       LastEditDate:chararray,LastEditorDisplayName:chararray,LastEditorUserId:chararray,\n",
    "       OwnerDisplayName:chararray,OwnerUserId:chararray,ParentId:chararray,\n",
    "       PostTypeId:chararray,Score:chararray,Tags:chararray,Title:chararray,ViewCount:chararray);\n",
    "ILLUSTRATE A;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código coge la misma información que hemos almacenado en la tabla HBase `posts`. Sólo se cogen un conjunto limitado de columnas y se muestra cómo se puede usar el tipo mapa de Pig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register '/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.7.0.jar';\n",
    "register '/usr/lib/hbase/hbase-client-1.2.0-cdh5.7.0.jar';\n",
    "register '/usr/lib/hbase/hbase-common-1.2.0-cdh5.7.0.jar';\n",
    "\n",
    "raw = LOAD 'hbase://posts'\n",
    "       USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(\n",
    "       'rawdata:Body rawdata:OwnerUserId rawdata:*', '-loadKey true -limit 5')\n",
    "       AS (Id:chararray, Body:chararray, OwnerUserId:chararray, rawdata:map[]);\n",
    "\n",
    "DUMP raw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código relaciona a la tabla usuarios de HBase con los Posts obtenidos de un fichero CSV. Lista los usuarios qué más entradas (preguntas+respuestas) tienen, ordenados por número de posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register '/usr/lib/zookeeper/zookeeper-3.4.5-cdh5.7.0.jar';\n",
    "register '/usr/lib/hbase/hbase-client-1.2.0-cdh5.7.0.jar';\n",
    "register '/usr/lib/hbase/hbase-common-1.2.0-cdh5.7.0.jar';\n",
    "register '/usr/lib/pig/piggybank.jar';\n",
    "\n",
    "define CSVLoader org.apache.pig.piggybank.storage.CSVLoader();\n",
    "\n",
    "-- Cargar Posts del fichero CSV\n",
    "Posts = LOAD '/user/cloudera/Posts.csv' using CSVLoader \n",
    "   AS (Id,AcceptedAnswerId,AnswerCount,Body,\n",
    "       ClosedDate,CommentCount,CommunityOwnedDate,\n",
    "       CreationDate,FavoriteCount,LastActivityDate,\n",
    "       LastEditDate,LastEditorDisplayName,LastEditorUserId,\n",
    "       OwnerDisplayName,OwnerUserId,ParentId,\n",
    "       PostTypeId,Score,Tags,Title,ViewCount);\n",
    "\n",
    "-- Cargar Users de HBase\n",
    "Users = LOAD 'hbase://users'\n",
    "       USING org.apache.pig.backend.hadoop.hbase.HBaseStorage(\n",
    "       'rawdata:AboutMe rawdata:AccountId rawdata:Age rawdata:CreationDate rawdata:DisplayName rawdata:DownVotes rawdata:LastAccessDate rawdata:Location rawdata:ProfileImageUrl rawdata:Reputation rawdata:UpVotes rawdata:Views rawdata:WebsiteUrl'\n",
    "        , '-loadKey true')\n",
    "       AS (Id,AboutMe,AccountId,Age:int,\n",
    "           CreationDate,DisplayName,DownVotes,\n",
    "           LastAccessDate,Location,ProfileImageUrl,\n",
    "           Reputation,UpVotes,Views,WebsiteUrl);\n",
    "\n",
    "ILLUSTRATE Users;\n",
    "\n",
    "PostByUser = GROUP Posts BY OwnerUserId;\n",
    "ILLUSTRATE PostByUser;\n",
    "\n",
    "PostByUser = FOREACH PostByUser GENERATE group as userId, COUNT($1) AS n;\n",
    "\n",
    "MaxPostByUser = FILTER PostByUser BY n >= 150;\n",
    "DUMP MaxPostByUser;\n",
    "\n",
    "Result = JOIN MaxPostByUser by userId, Users by Id;\n",
    "Result = FOREACH Result GENERATE userId, DisplayName, n;\n",
    "Result = ORDER Result BY n DESC;\n",
    "\n",
    "DUMP Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
